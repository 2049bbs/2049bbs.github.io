---
aid: 1166
cid: 2
authorID: 1227
addTime: 2019-05-09T13:42:00.000Z
title: 今天又被知乎恶心到了，然后有个想法
tags:
    - 想法
comments:
    -
        authorID: 2157
        addTime: 2019-11-02T16:00:00.000Z
        content: |-
            > 比如联系下品葱的那群大佬。

            现今知道了榴梿就是小二，再看这句话，真是忍俊不禁，忍不住挖个坟。
    -
        authorID: 1
        addTime: 2019-11-02T16:00:00.000Z
        content: 爬知乎貌似很容易啊，一堆现成的轮子。不过怕就怕在三天两头被封ip，这就很烦。
    -
        authorID: 2351
        addTime: 2019-11-02T16:00:00.000Z
        content: >-
            [“知乎回答”被认定有著作权
            未经许可使用须赔偿](https://news.sina.com.cn/s/2019-07-03/doc-ihytcitk9506187.shtml)
    -
        authorID: 2355
        addTime: 2019-11-03T16:00:00.000Z
        content: >-
            @[懦夫斯基](/member/%E6%87%A6%E5%A4%AB%E6%96%AF%E5%9F%BA) #1
            不懂就问：之前本站和pincong发生过什么事儿吗，榴莲为什么是小二？
    -
        authorID: 2202
        addTime: 2019-11-03T16:00:00.000Z
        content: >-
            @[millionray](/member/millionray) #4
            参见此帖[https://2049bbs.xyz/t/1482](https://2049bbs.xyz/t/1482)
    -
        authorID: 2826
        addTime: 2020-01-22T22:45:00.000Z
        content: 知乎是小布尔乔亚大本营，有水平的见解远远比你想象得要少。爬那些抖机灵的回答没意义。
    -
        authorID: 2762
        addTime: 2020-01-23T01:15:00.000Z
        content: 现在的知乎真的跟贴吧一样了
date: 2020-01-23T01:15:00.000Z
category: 时政
---

关于定时爬取知乎热门社会性事件问题下的内容

经常发生社会性新闻后，知乎上的东西就会被删掉。关键是被删的回答和问题总是很“精华”的。 因此我在想，既然全站爬取知乎内容比较难，为何不搞一个项目专门在热点事件发生时，第一时间让爬虫去监控对应的知乎问题下的所有内容呢？

然后把数据公开。比如利用Github API存到Github上。当然，提供良好的前端页面的话，就更好了。

类似这个项目：[http://206.189.252.32:3838/Wechatscope/](http://206.189.252.32:3838/Wechatscope/) [http://206.189.252.32:3838/](http://206.189.252.32:3838/)

这个项目还算比较有意义吧。怎么样，有没有技术大佬搞的？ 我也把这个想法发到其他地方，比如联系下品葱的那群大佬。
