---
aid: 6250
cid: 8
authorID: 3619
addTime: 2020-07-11T07:20:00.000Z
title: 【霏艺所思】霏藝眼裏的大數據
tags:
    - 所思
comments: []
date: 2020-07-11T07:20:00.000Z
category: 博客
---

**我不是做大數據的，所以都是外行下琢磨而已**

### [](#%E5%A4%9A%E5%A4%A7%E7%9A%84%E6%95%B8%E6%93%9A%E7%AE%97%E5%A4%A7%E6%95%B8%E6%93%9A)多大的數據算大數據？

三個情況：

1.  單機存不下的**單個大文件**【比如10TB的超級大文件】
    
2.  單機存不下的**超多數量文件**【比如几千億張圖片】
    
3.  單機處理不了的數據量
    

一般把單機解決不了的問題，定義為大數據。

所以大數據可以分爲兩個問題：“數據怎麽存”和“數據怎麽用”

### [](#%E4%BB%80%E9%BA%BD%E6%98%AFmapreduce)什麽是mapreduce？

我們從數據怎麽用開始講起吧~

比如，我現在有一組數據，我想找到這個數據裏的最小值

我們把前一個數據的結果和後面這個數據比較，這個過程就是reduce了

    比如 sum = 0， sum = sum + element[i]
    

這個就是最基本的reduce

再比如，我有一個城市所有人的數據，

我知道他們的出生年月，現在要計算他們的年紀 = 2020 - 他們的出生年

這個過程就是map了

    birthyear_list = 保存了所有人的出生年份，list結構
    age_map(list) =  2020 - list 計算list的年紀，遍歷list每個元素，執行這個map函數
    

現在連起來思考，我得到了整個城市所有人的出生年份

現在計算誰的年紀最大。

map函數計算大家的年份，reduce函數計算誰的年紀最大

    所有人的出生年份.mapper(2020-birth).reduce(a>b?a:b)
    

mapreduce，講解完畢~

### [](#%E7%88%B2%E4%BB%80%E9%BA%BD%E5%88%97%E5%BC%8F%E5%AD%98%E5%84%B2)爲什麽列式存儲？

大家都看到了，map和reduce的入參，是一個**數據類型相同**的list！

如果是傳統的行式存儲，我需要這麽寫代碼

    // 查找整個城市的所有人裏年紀最大的人的歲數
    // table 表示 一個表 存了所有人的個人信息，出生年份記錄在第七列
    list = []
    遍歷 table :
           讀取一行數據 放到 row
           讀取row的第7列 ，放到year
          list.append(year)
    
    list.map(2020-year).reduce(a>b?a:b)
    
    

再來看看列式存儲

    // 查找整個城市的所有人裏年紀最大的人的歲數
    // table 表示 一個表 存了所有人的個人信息，出生年份記錄在第七列
    table[7].map(2020-year).reduce(a>b?a:b)
    

因爲列式儲存，所以可以直接拿出一列的數據【就像行式存儲，每次拿到一行數據】 所以，列式存儲，非常適合map reduce操作！！！【省時間，省空間，速度非常快】

### [](#%E5%A4%A7%E6%95%B8%E6%93%9A%E6%80%8E%E9%BA%BD%E5%AD%98)大數據怎麽存？

Google爲了解決數據的存儲問題，首先就構建了一個分佈式存儲文件系統GFS

這樣就可以存很大很大的文件了【數據庫文件】

光是裸數據，不方便使用啊。所以要用數據庫來管理。

然後爲了後續使用方便，所以在分佈式文件系統GFS上，開發了列式存儲的數據庫BigTable

開源項目Hadoop，分別實現了GFS和BigTable【取名字是HDFS和HBase】

### [](#%E5%A4%A7%E6%95%B8%E6%93%9A%E6%80%8E%E9%BA%BD%E7%94%A8)大數據怎麽用？

關於數據怎麽用，就是我説的mapreduce了，Hadoop也實現了Mapreduce

不過，現在已經被Spark替代了

對數據map以後，分給n多個小電腦，分別處理，在統一回收，得到新的list

把list再給reduce，整個過程都是分佈式的

spark是一種特殊的存在，主要優勢就是内存計算【可以交互】

mapreduce看上去很簡單，實際也很簡單，用map reduce寫代碼，就像匯編寫程序，你説能不能幹活，肯定可以！但是效率太慢了，寫起來囉嗦！還容易寫錯！

所以就想，能不能搞點高級語言？

然後封裝了Pig和Hive【Hive可以實現類似SQL的方式操作分佈式存儲系統】

爲什麽會這樣呢？

數據庫可以大致分爲 OLTP和 OLAP

OLTP就是，我們常説的CRUD，説到CRUD自然想到 SQL，所以我們覺得可以用SQL來寫分佈式計算，滿足一定的需求【Spark SQL 應運而生】

OLAP就是，我們常説的統計，分析，聚類。。。然後説到統計，我們就想到了R語言。然後SparkR就出現 了~

分佈式除了儅數據庫，還可以用來做很多事情

比如神經網絡，訓練需要很多計算資源，用Spark可以實現分佈式計算，MLlib

再比如圖數據庫，GraphX

計算24小時的熱詞，搜索等等 Spark Streaming

spark生態系統就這麽一堆東西出來了~

Spark可以跑在集群上，比如yarn ，k8s

數據源可以是HBase，Hive，Cassandra等等。。。

### [](#%E6%A0%B9%E6%93%9A%E6%95%B8%E6%93%9A%E7%B5%90%E6%A7%8B%E5%88%86%E9%A1%9E%E6%95%B8%E6%93%9A%E5%BA%AB)根據數據結構分類數據庫

如果說程序 就是 數據結構 + 算法

那麽數據庫 就是 存儲結構 + 計算引擎

計算引擎 現在基本就是 Spark了

那麽存儲結構有哪些呢？

1.  Vector結構 ： 時序數據庫 influxdb
    
2.  List 結構 ： 區塊鏈
    
3.  Map結構 ： Redis
    
4.  Tree結構： MongoDB
    
5.  Table結構 ： MySQL
    
6.  Graph結構： Neo4J
