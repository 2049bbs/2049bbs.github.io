---
aid: 6250
cid: 8
authorID: 3619
addTime: 2020-07-11T07:20:00.000Z
title: 【霏艺所思】霏藝眼裏的大數據
tags:
    - 所思
comments:
    -
        authorID: 3619
        addTime: 2020-07-11T08:20:00.000Z
        content: >-
            感覺大數據被雲計算衝擊的很厲害


            大數據的存儲，被雲存儲替代了


            大數據的計算，也有k8s和 虛擬化解決方案


            感覺，神經網絡，也可以用MPI去實現。


            就是感覺 ，現在 還説大數據，就不知道應用點在哪裏？


            * * *


            區塊鏈，感覺熱度下去了吧？


            一致性算法，感覺Raft ， Paxos更多些，真有人用Pow去做一致性麽？


            * * *


            VR好像熱度也下去了


            Google 好像不搞VR了？


            不知道現在的WebXR是什麽情況


            * * *


            我感覺，IT行業泡沫真大啊。。。


            也許是我不懂吧


            不是很喜歡現在，特別浮躁，動不動就一個概念


            沒有人踏踏實實做技術


            感覺年輕人都是兩個極端


            1.  注重論文，看了很多，寫了很多，就是不肯寫代碼【光說理論，紙上談兵的感覺】
                
            2. 
            不看論文，光是跟風。天天說“這個死了”，“那個主流”。代碼水平也許很好吧，也看了很多開源項目。但是技術沒有任何沉澱，也不是很懂算法。感覺就是跟風使用各種開源項目。
                

            * * *


            紙上得來終覺淺，絕知此事要躬行


            做學術，不能浮躁。先去想想，我要解決什麽問題。給自己立下一個課題。


            然後，開始學習相關的技術和論文。


            最後開始自己，找個解決辦法。用編程，用算法去實現。


            這個時候，就是踩坑了！就是踩坑的時候，我們才知道，我們需要做什麽細節。


            也就是這些細節，是最寶貴的經驗。因爲這些細節。我們的論文才有價值。


            比如Relu這個激活函數，就是在事件中發現的。


            當然，後來，我們都搞gelu了


            然後sgd也換成了adam


            但是，這些細節，都需要我們去實踐！


            不去實踐，你永遠發現不了自己該去幹什麽


            光寫論文是沒用的，還是需要去寫代碼，實現別人的論文


            或者實現自己的論文，發現一些不足，自己要去改進


            所以，這個才是做學術的態度
    -
        authorID: 3793
        addTime: 2020-07-11T08:20:00.000Z
        content: |-
            感谢分享。

            话说，我觉得“大数据”这个词多年来被媒体严重滥用了。
    -
        authorID: 3619
        addTime: 2020-07-12T15:20:00.000Z
        content: |-
            假設銀行記錄了每條客戶操作記錄為100字節

            每天100條

            那麽一年大概消耗4MB的硬盤

            假設，銀行用戶10億，每年可以產生4PB的用戶數據

            考慮到雙活，那麽預估，中國的銀行，每年硬盤數據增長就高達10PB

            * * *

            考慮嗶哩嗶哩

            每條用戶操作都記錄數據庫

            每條記錄100字節，每天100條，每個用戶一年可以產生4MB的數據

            假設用戶4億人，那麽1.6PB，考慮3副本備份，那麽就是5PB

            * * *

            所以，“金融，通信，交通，互聯網”行業在數據存儲問題上，一定會遇到大數據的問題。

            通過以前 SAN ，陣列等存儲方式，必然有很高的成本問題，通過分佈式文件系統可以緩解

            但是現在出現了更加便宜的雲存儲方案
date: 2020-07-12T15:20:00.000Z
category: 博客
---

**我不是做大數據的，所以都是外行下琢磨而已**

### [](#%E5%A4%9A%E5%A4%A7%E7%9A%84%E6%95%B8%E6%93%9A%E7%AE%97%E5%A4%A7%E6%95%B8%E6%93%9A)多大的數據算大數據？

三個情況：

1.  單機存不下的**單個大文件**【比如10TB的超級大文件】
    
2.  單機存不下的**超多數量文件**【比如几千億張圖片】
    
3.  單機處理不了的數據量
    

一般把單機解決不了的問題，定義為大數據。

所以大數據可以分爲兩個問題：“數據怎麽存”和“數據怎麽用”

### [](#%E4%BB%80%E9%BA%BD%E6%98%AFmapreduce)什麽是mapreduce？

我們從數據怎麽用開始講起吧~

比如，我現在有一組數據，我想找到這個數據裏的最小值

我們把前一個數據的結果和後面這個數據比較，這個過程就是reduce了

    比如 sum = 0， sum = sum + element[i]
    

這個就是最基本的reduce

再比如，我有一個城市所有人的數據，

我知道他們的出生年月，現在要計算他們的年紀 = 2020 - 他們的出生年

這個過程就是map了

    birthyear_list = 保存了所有人的出生年份，list結構
    age_map(list) =  2020 - list 計算list的年紀，遍歷list每個元素，執行這個map函數
    

現在連起來思考，我得到了整個城市所有人的出生年份

現在計算誰的年紀最大。

map函數計算大家的年份，reduce函數計算誰的年紀最大

    所有人的出生年份.mapper(2020-birth).reduce(a>b?a:b)
    

mapreduce，講解完畢~

### [](#%E7%88%B2%E4%BB%80%E9%BA%BD%E5%88%97%E5%BC%8F%E5%AD%98%E5%84%B2)爲什麽列式存儲？

大家都看到了，map和reduce的入參，是一個**數據類型相同**的list！

如果是傳統的行式存儲，我需要這麽寫代碼

    // 查找整個城市的所有人裏年紀最大的人的歲數
    // table 表示 一個表 存了所有人的個人信息，出生年份記錄在第七列
    list = []
    遍歷 table :
           讀取一行數據 放到 row
           讀取row的第7列 ，放到year
          list.append(year)
    
    list.map(2020-year).reduce(a>b?a:b)
    
    

再來看看列式存儲

    // 查找整個城市的所有人裏年紀最大的人的歲數
    // table 表示 一個表 存了所有人的個人信息，出生年份記錄在第七列
    table[7].map(2020-year).reduce(a>b?a:b)
    

因爲列式儲存，所以可以直接拿出一列的數據【就像行式存儲，每次拿到一行數據】 所以，列式存儲，非常適合map reduce操作！！！【省時間，省空間，速度非常快】

### [](#%E5%A4%A7%E6%95%B8%E6%93%9A%E6%80%8E%E9%BA%BD%E5%AD%98)大數據怎麽存？

Google爲了解決數據的存儲問題，首先就構建了一個分佈式存儲文件系統GFS

這樣就可以存很大很大的文件了【數據庫文件】

光是裸數據，不方便使用啊。所以要用數據庫來管理。

然後爲了後續使用方便，所以在分佈式文件系統GFS上，開發了列式存儲的數據庫BigTable

開源項目Hadoop，分別實現了GFS和BigTable【取名字是HDFS和HBase】

### [](#%E5%A4%A7%E6%95%B8%E6%93%9A%E6%80%8E%E9%BA%BD%E7%94%A8)大數據怎麽用？

關於數據怎麽用，就是我説的mapreduce了，Hadoop也實現了Mapreduce

不過，現在已經被Spark替代了

對數據map以後，分給n多個小電腦，分別處理，在統一回收，得到新的list

把list再給reduce，整個過程都是分佈式的

spark是一種特殊的存在，主要優勢就是内存計算【可以交互】

mapreduce看上去很簡單，實際也很簡單，用map reduce寫代碼，就像匯編寫程序，你説能不能幹活，肯定可以！但是效率太慢了，寫起來囉嗦！還容易寫錯！

所以就想，能不能搞點高級語言？

然後封裝了Pig和Hive【Hive可以實現類似SQL的方式操作分佈式存儲系統】

爲什麽會這樣呢？

數據庫可以大致分爲 OLTP和 OLAP

OLTP就是，我們常説的CRUD，説到CRUD自然想到 SQL，所以我們覺得可以用SQL來寫分佈式計算，滿足一定的需求【Spark SQL 應運而生】

OLAP就是，我們常説的統計，分析，聚類。。。然後説到統計，我們就想到了R語言。然後SparkR就出現 了~

分佈式除了儅數據庫，還可以用來做很多事情

比如神經網絡，訓練需要很多計算資源，用Spark可以實現分佈式計算，MLlib

再比如圖數據庫，GraphX

計算24小時的熱詞，搜索等等 Spark Streaming

spark生態系統就這麽一堆東西出來了~

Spark可以跑在集群上，比如yarn ，k8s

數據源可以是HBase，Hive，Cassandra等等。。。

### [](#%E6%A0%B9%E6%93%9A%E6%95%B8%E6%93%9A%E7%B5%90%E6%A7%8B%E5%88%86%E9%A1%9E%E6%95%B8%E6%93%9A%E5%BA%AB)根據數據結構分類數據庫

如果說程序 就是 數據結構 + 算法

那麽數據庫 就是 存儲結構 + 計算引擎

計算引擎 現在基本就是 Spark了

那麽存儲結構有哪些呢？

1.  Vector結構 ： 時序數據庫 influxdb
    
2.  List 結構 ： 區塊鏈
    
3.  Map結構 ： Redis
    
4.  Tree結構： MongoDB
    
5.  Table結構 ： MySQL
    
6.  Graph結構： Neo4J
